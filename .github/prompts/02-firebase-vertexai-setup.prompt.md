# Phase 1, Step 2: Firebase & Vertex AI Configuration

## Context & Requirements
Configure Firebase and Vertex AI for production-ready AI photo editor app with proper security, error handling, and scalability. This is critical for the PROMPTER (Gemini 2.5) → EDITOR (Google Imagen) pipeline.

**Critical Technical Requirements:**
- Firebase: Latest SDK with proper initialization
- Vertex AI: Gemini 2.5 Pro + Google Imagen models
- Security: Environment-based API key management
- Error handling: Comprehensive Firebase exception handling
- Performance: Connection pooling and retry logic
- Testing: Mock Firebase for unit tests

## Exact Implementation Specifications

### 1. Firebase Project Setup Commands
```bash
# Install Firebase CLI (if not installed)
npm install -g firebase-tools

# Login and initialize
firebase login
firebase init

# Select:
# - Functions: Configure a Cloud Functions directory
# - Hosting: Configure files for Firebase Hosting
# - Storage: Configure a Cloud Storage bucket
# - Emulators: Set up local emulators

# Install FlutterFire CLI
dart pub global activate flutterfire_cli

# Configure Firebase for Flutter
flutterfire configure
```

### 2. Firebase Configuration Files
```dart
// lib/firebase_options.dart - Auto-generated by FlutterFire
import 'package:firebase_core/firebase_core.dart' show FirebaseOptions;
import 'package:flutter/foundation.dart'
    show defaultTargetPlatform, kIsWeb, TargetPlatform;

class DefaultFirebaseOptions {
  static FirebaseOptions get currentPlatform {
    if (kIsWeb) return web;
    switch (defaultTargetPlatform) {
      case TargetPlatform.android:
        return android;
      case TargetPlatform.iOS:
        return ios;
      default:
        throw UnsupportedError(
          'DefaultFirebaseOptions have not been configured for this platform.',
        );
    }
  }

  // Platform-specific configurations will be auto-generated
  static const FirebaseOptions web = FirebaseOptions(/* config */);
  static const FirebaseOptions android = FirebaseOptions(/* config */);
  static const FirebaseOptions ios = FirebaseOptions(/* config */);
}
```

### 3. Environment-Based Configuration
```dart
// lib/core/constants/firebase_constants.dart
import '../constants/app_constants.dart';

class FirebaseConstants {
  static String get projectId {
    switch (AppConstants.environment) {
      case Environment.development:
        return 'ai-photo-editor-dev';
      case Environment.staging:
        return 'ai-photo-editor-staging';
      case Environment.production:
        return 'ai-photo-editor-prod';
    }
  }

  static String get vertexAiLocation => 'us-central1';
  
  static String get geminiModel => 'gemini-2.0-flash-exp';
  static String get imagenModel => 'imagen-3.0-generate-001';
  
  // Timeouts and limits
  static const Duration aiRequestTimeout = Duration(seconds: 60);
  static const int maxRetryAttempts = 3;
  static const Duration retryDelay = Duration(seconds: 2);
  
  // Image processing limits
  static const int maxImageSize = 4096; // 4K max
  static const int maxFileSizeMB = 10;
  static const List<String> supportedFormats = ['jpg', 'jpeg', 'png', 'heic'];
}
```

### 4. Enhanced Bootstrap Configuration
```dart
// lib/bootstrap.dart - Update with Firebase initialization
import 'dart:async';
import 'dart:developer';

import 'package:firebase_core/firebase_core.dart';
import 'package:firebase_auth/firebase_auth.dart';
import 'package:firebase_vertexai/firebase_vertexai.dart';
import 'package:flutter/widgets.dart';
import 'package:flutter_bloc/flutter_bloc.dart';
import 'package:hive_flutter/hive_flutter.dart';

import 'app/app.dart';
import 'core/di/injection_container.dart' as di;
import 'core/constants/firebase_constants.dart';
import 'core/services/analytics_service.dart';
import 'core/services/crash_reporting_service.dart';
import 'firebase_options.dart';

class AppBlocObserver extends BlocObserver {
  const AppBlocObserver();

  @override
  void onChange(BlocBase<dynamic> bloc, Change<dynamic> change) {
    super.onChange(bloc, change);
    log('${bloc.runtimeType} $change');
  }

  @override
  void onError(BlocBase<dynamic> bloc, Object error, StackTrace stackTrace) {
    log('${bloc.runtimeType} $error $stackTrace');
    super.onError(bloc, error, stackTrace);
  }
}

Future<void> bootstrap(FutureOr<Widget> Function() builder) async {
  FlutterError.onError = (details) {
    log(details.exceptionAsString(), stackTrace: details.stack);
    CrashReportingService.recordFlutterError(details);
  };

  Bloc.observer = const AppBlocObserver();

  // Initialize core services
  await _initializeCoreServices();
  
  // Initialize dependency injection
  await di.init();

  runApp(await builder());
}

Future<void> _initializeCoreServices() async {
  try {
    // Initialize Hive for local storage
    await Hive.initFlutter();
    
    // Initialize Firebase
    await Firebase.initializeApp(
      options: DefaultFirebaseOptions.currentPlatform,
    );
    
    // Initialize Vertex AI
    await _initializeVertexAI();
    
    // Initialize analytics
    await AnalyticsService.initialize();
    
    log('Core services initialized successfully');
  } catch (e, stackTrace) {
    log('Failed to initialize core services: $e', stackTrace: stackTrace);
    await CrashReportingService.recordError(e, stackTrace);
    rethrow;
  }
}

Future<void> _initializeVertexAI() async {
  try {
    final vertexAI = FirebaseVertexAI.instance;
    
    // Test connection with a simple health check
    final model = vertexAI.generativeModel(
      model: FirebaseConstants.geminiModel,
      systemInstruction: Content.system('Health check'),
    );
    
    // Verify model is accessible (don't actually generate content)
    log('Vertex AI initialized successfully');
  } catch (e, stackTrace) {
    log('Vertex AI initialization failed: $e', stackTrace: stackTrace);
    // Don't rethrow - app should work without AI initially
  }
}
```

### 5. Vertex AI Service Implementation
```dart
// lib/core/services/vertex_ai_service.dart
import 'dart:convert';
import 'dart:typed_data';

import 'package:firebase_vertexai/firebase_vertexai.dart';
import 'package:image/image.dart' as img;

import '../constants/firebase_constants.dart';
import '../error/exceptions.dart';

class VertexAIService {
  late final GenerativeModel _geminiModel;
  late final GenerativeModel _imagenModel;
  
  VertexAIService() {
    final vertexAI = FirebaseVertexAI.instance;
    
    _geminiModel = vertexAI.generativeModel(
      model: FirebaseConstants.geminiModel,
      generationConfig: GenerationConfig(
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 2048,
      ),
    );
    
    _imagenModel = vertexAI.generativeModel(
      model: FirebaseConstants.imagenModel,
      generationConfig: GenerationConfig(
        temperature: 0.4,
        maxOutputTokens: 1024,
      ),
    );
  }

  /// Generate editing instructions using Gemini 2.5 Pro
  Future<String> generateEditingPrompt({
    required Uint8List imageBytes,
    required List<Map<String, dynamic>> markers,
    int? retryCount = 0,
  }) async {
    try {
      // Validate image
      await _validateImage(imageBytes);
      
      // Create content with image and markers
      final content = [
        Content.multi([
          DataPart('image/jpeg', imageBytes),
          TextPart(_buildPromptText(markers)),
        ])
      ];

      final response = await _geminiModel.generateContent(content);
      
      if (response.text == null || response.text!.isEmpty) {
        throw AIProcessingException('Empty response from Gemini model');
      }
      
      return response.text!;
      
    } catch (e) {
      if (retryCount! < FirebaseConstants.maxRetryAttempts) {
        await Future.delayed(
          Duration(seconds: FirebaseConstants.retryDelay.inSeconds * (retryCount + 1)),
        );
        return generateEditingPrompt(
          imageBytes: imageBytes,
          markers: markers,
          retryCount: retryCount + 1,
        );
      }
      
      throw AIProcessingException('Failed to generate editing prompt: $e');
    }
  }

  /// Process image with Google Imagen
  Future<Uint8List> processImageWithAI({
    required Uint8List imageBytes,
    required String editingPrompt,
    int? retryCount = 0,
  }) async {
    try {
      // Validate image
      await _validateImage(imageBytes);
      
      // Create content for image generation
      final content = [
        Content.multi([
          DataPart('image/jpeg', imageBytes),
          TextPart(editingPrompt),
        ])
      ];

      final response = await _imagenModel.generateContent(content);
      
      // Extract image data from response
      if (response.candidates.isEmpty) {
        throw AIProcessingException('No image generated by Imagen model');
      }
      
      // Handle the response based on actual Imagen response format
      // Note: This will need adjustment based on actual Imagen API response
      final candidate = response.candidates.first;
      if (candidate.content.parts.isEmpty) {
        throw AIProcessingException('Empty image response from Imagen');
      }
      
      // Extract image bytes (implementation depends on actual response format)
      return _extractImageFromResponse(candidate);
      
    } catch (e) {
      if (retryCount! < FirebaseConstants.maxRetryAttempts) {
        await Future.delayed(
          Duration(seconds: FirebaseConstants.retryDelay.inSeconds * (retryCount + 1)),
        );
        return processImageWithAI(
          imageBytes: imageBytes,
          editingPrompt: editingPrompt,
          retryCount: retryCount + 1,
        );
      }
      
      throw AIProcessingException('Failed to process image with AI: $e');
    }
  }

  String _buildPromptText(List<Map<String, dynamic>> markers) {
    return '''
You are a professional photo editor AI. Analyze this image and the marked objects for seamless removal.

Marked objects for removal: ${markers.map((m) => 'Object at normalized position (${m['x']}, ${m['y']})').join(', ')}

Instructions for image editing:
1. Analyze lighting conditions around marked areas
2. Identify background patterns and textures  
3. Generate content-aware fill considering:
   - Shadow consistency and natural lighting
   - Texture matching and pattern continuation
   - Color harmonization with surrounding areas
   - Perspective accuracy and depth
4. Ensure no artifacts or obvious editing traces
5. Maintain photo realism and natural appearance
6. Preserve image quality and resolution

Generate precise editing instructions for seamless object removal.
''';
  }

  Future<void> _validateImage(Uint8List imageBytes) async {
    // Check file size
    if (imageBytes.length > FirebaseConstants.maxFileSizeMB * 1024 * 1024) {
      throw ImageProcessingException(
        'Image too large: ${imageBytes.length ~/ (1024 * 1024)}MB. Max: ${FirebaseConstants.maxFileSizeMB}MB',
      );
    }

    // Decode and validate image
    final image = img.decodeImage(imageBytes);
    if (image == null) {
      throw ImageProcessingException('Invalid image format');
    }

    // Check dimensions
    if (image.width > FirebaseConstants.maxImageSize || 
        image.height > FirebaseConstants.maxImageSize) {
      throw ImageProcessingException(
        'Image too large: ${image.width}x${image.height}. Max: ${FirebaseConstants.maxImageSize}x${FirebaseConstants.maxImageSize}',
      );
    }
  }

  Uint8List _extractImageFromResponse(GenerateContentCandidate candidate) {
    // Implementation depends on actual Imagen response format
    // This is a placeholder that needs to be updated based on real API
    
    for (final part in candidate.content.parts) {
      if (part is DataPart && part.mimeType.startsWith('image/')) {
        return part.bytes;
      }
    }
    
    throw AIProcessingException('No image data found in AI response');
  }
}
```

### 6. Firebase Security Rules
```javascript
// storage.rules - Cloud Storage security rules
rules_version = '2';
service firebase.storage {
  match /b/{bucket}/o {
    // Images bucket - authenticated users only
    match /images/{userId}/{imageId} {
      allow read, write: if request.auth != null 
        && request.auth.uid == userId
        && imageId.matches('.*\\.(jpg|jpeg|png|heic)$')
        && resource.size < 10 * 1024 * 1024; // 10MB limit
    }
    
    // Processed images bucket
    match /processed/{userId}/{imageId} {
      allow read, write: if request.auth != null 
        && request.auth.uid == userId;
    }
  }
}

// firestore.rules - Firestore security rules
rules_version = '2';
service cloud.firestore {
  match /databases/{database}/documents {
    // User data
    match /users/{userId} {
      allow read, write: if request.auth != null 
        && request.auth.uid == userId;
    }
    
    // Processing jobs
    match /processing_jobs/{jobId} {
      allow read, write: if request.auth != null 
        && resource.data.userId == request.auth.uid;
    }
  }
}
```

### 7. Error Handling Extensions
```dart
// lib/core/error/exceptions.dart - Add Firebase-specific exceptions
abstract class AppException implements Exception {
  const AppException(this.message, [this.code]);
  
  final String message;
  final String? code;
  
  @override
  String toString() => 'AppException: $message${code != null ? ' (Code: $code)' : ''}';
}

class NetworkException extends AppException {
  const NetworkException(super.message, [super.code]);
}

class StorageException extends AppException {
  const StorageException(super.message, [super.code]);
}

class AuthenticationException extends AppException {
  const AuthenticationException(super.message, [super.code]);
}

class ImageProcessingException extends AppException {
  const ImageProcessingException(super.message, [super.code]);
}

class AIProcessingException extends AppException {
  const AIProcessingException(super.message, [super.code]);
}

// Firebase-specific exceptions
class FirebaseInitializationException extends AppException {
  const FirebaseInitializationException(super.message, [super.code]);
}

class VertexAIException extends AppException {
  const VertexAIException(super.message, [super.code]);
}

class QuotaExceededException extends AppException {
  const QuotaExceededException(super.message, [super.code]);
}
```

### 8. Testing Configuration
```dart
// test/helpers/firebase_test_helper.dart
import 'package:firebase_core/firebase_core.dart';
import 'package:firebase_vertexai/firebase_vertexai.dart';
import 'package:mocktail/mocktail.dart';

class MockFirebaseApp extends Mock implements FirebaseApp {}
class MockGenerativeModel extends Mock implements GenerativeModel {}
class MockGenerateContentResponse extends Mock implements GenerateContentResponse {}

class FirebaseTestHelper {
  static Future<void> setupFirebaseForTesting() async {
    // Setup mock Firebase for testing
    Firebase.delegatePackingProperty = MockFirebaseApp();
  }
  
  static void setupVertexAIMocks() {
    // Setup Vertex AI mocks for testing
    registerFallbackValue(Content.text(''));
    registerFallbackValue(GenerationConfig());
  }
}
```

### 9. Environment Variables Template
```bash
# .env.development
FIREBASE_PROJECT_ID=ai-photo-editor-dev
VERTEX_AI_LOCATION=us-central1
GEMINI_MODEL=gemini-2.0-flash-exp
IMAGEN_MODEL=imagen-3.0-generate-001

# .env.production  
FIREBASE_PROJECT_ID=ai-photo-editor-prod
VERTEX_AI_LOCATION=us-central1
GEMINI_MODEL=gemini-2.0-flash-exp
IMAGEN_MODEL=imagen-3.0-generate-001
```

## Acceptance Criteria (Must All Pass)
1. ✅ Firebase initializes successfully in all environments
2. ✅ Vertex AI models are accessible and tested
3. ✅ Security rules protect user data appropriately
4. ✅ Error handling covers all Firebase scenarios
5. ✅ Image validation prevents oversized uploads
6. ✅ Retry logic handles transient failures
7. ✅ Environment-based configuration works
8. ✅ Tests can mock Firebase services
9. ✅ API quotas and limits are enforced
10. ✅ Performance meets requirements (< 60s processing)

**Implementation Priority:** Critical dependency for all AI features

**Quality Gate:** All Firebase services initialize without errors

**Performance Target:** AI processing completes within 60 seconds

---

**Next Step:** After completion, proceed to Authentication Domain Layer (Phase 2, Step 3)
